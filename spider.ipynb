{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "res=requests.get('http://news.sina.com.cn/c/nd/2018-06-08/doc-ihcscwxa1809510.shtml')\n",
    "res.encoding='utf-8'\n",
    "soup=BeautifulSoup(res.text,'html.parser')    \n",
    "title=soup.select('.main-title')[0].text\n",
    "#timesource1=soup.select('.date-source')[0].text.split('\\n')[1]    #获取时间\n",
    "timesource=soup.select('.date-source span')[0].text         #获取时间\n",
    "dt=datetime.strptime(timesource,'%Y年%m月%d日 %H:%M')\n",
    "dt.strftime('%Y-%m-%d')\n",
    "place=soup.select('.date-source a')[0].text    #获取新闻来源\n",
    "article=[]                                   #获取文章内容\n",
    "for p in soup.select('#article p')[:-1]:\n",
    "    article.append(p.text.strip())\n",
    "articleall=' '.join(article)\n",
    "editor=soup.select('#article p')[-1].text.strip('责任编辑：')     #获取作者姓名\n",
    "comments=requests.get('http://comment5.news.sina.com.cn/page/info?version=1&format=json&\\\n",
    "channel=gn&newsid=comos-hcscwxa1809510&group=undefined&compress=0&ie=utf-8&\\\n",
    "oe=utf-8&page=1&page_size=3&t_size=3&h_size=3&thread=1')                      \n",
    "#print(comments.text)    \n",
    "jd=json.loads(comments.text)         #用jason解析器\n",
    "comment_num=jd['result']['count']['total']        #获得评论总数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#newsurl='http://news.sina.com.cn/c/nd/2018-06-08/doc-ihcscwxa1809510.shtml'\n",
    "\n",
    "newsurl = 'https://news.sina.com.cn/c/zj/2019-06-12/doc-ihvhiqay5255597.shtml'\n",
    "commenturl='http://comment5.news.sina.com.cn/page/info?version=1&format=json&channel=gn&newsid=comos-{}&group=undefined&compress=0&ie=utf-8&oe=utf-8&page=1&page_size=3&t_size=3&h_size=3&thread=1'\n",
    "def getcommentcounts(newsurl):            #获取评论数\n",
    "    m=re.compile('doc-i(.*?).shtml').findall(newsurl)\n",
    "    newsid=m[0]\n",
    "    comments=requests.get(commenturl.format(newsid))\n",
    "    jd=json.loads(comments.text)\n",
    "    return jd['result']['count']['total']\n",
    " \n",
    "def getnewsdetail(newsurl):                                        #获得单页的新闻内容\n",
    "    result={}\n",
    "    res=requests.get(newsurl)\n",
    "    res.encoding='utf-8'\n",
    "    soup=BeautifulSoup(res.text,'html.parser')\n",
    "    result['title']=soup.select('.main-title')[0].text      #标题\n",
    "    timesource=soup.select('.date-source span')[0].text  \n",
    "    result['time']=datetime.strptime(timesource,'%Y年%m月%d日 %H:%M').strftime('%Y-%m-%d')              #时间\n",
    "    result['place']=soup.select('.source')[0].text       #来源\n",
    "    article=[]                                   #获取文章内容\n",
    "    for p in soup.select('#article p')[:-1]:\n",
    "        article.append(p.text.strip())\n",
    "    articleall=' '.join(article)\n",
    "    result['article']=articleall\n",
    "    result['editor']=soup.select('#article p')[-1].text.strip('责任编辑：')     #获取作者姓名\n",
    "    result['comment_num']=getcommentcounts(newsurl)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "getnewsdetail(newsurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_url = 'https://news.sina.com.cn/c/zj/2019-06-12/doc-ihvhiqay5255597.shtml'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url='http://api.roll.news.sina.com.cn/zt_list?channel=news&cat_1=gnxw&cat_2==gdxw1||=gatxw||=zs-pl||=mtjj&level==1||=2&show_ext=1&show_all=1&show_num=22&tag=1&format=json&page={}&callback=newsloadercallback&_=1528548757769'\n",
    "def parseListLinks(url):\n",
    "    newsdetail=[]\n",
    "    res=requests.get(url)\n",
    "    print(res.text)\n",
    "    jd=json.loads(res.text.lstrip('  newsloadercallback(').rstrip(');'))\n",
    "    for ent in jd['result']['data']:\n",
    "        newsdetail.append(getnewsdetail(ent['url']))\n",
    "    return newsdetail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news_total=[]\n",
    "for i in range(1,2):\n",
    "    newsurl=url.format(i)\n",
    "    newsary=parseListLinks(newsurl)\n",
    "    news_total.extend(newsary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame(news_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib import request\n",
    "def download(title, url,m):\n",
    "    req = request.Request(url)\n",
    "    response = request.urlopen(req)\n",
    "    response = response.read().decode('utf-8')\n",
    "    soup = BeautifulSoup(response,'lxml')\n",
    "    tag = soup.find('div',class_='article')\n",
    "    if tag == None:\n",
    "        return 0\n",
    "    #print(type(tag))\n",
    "    #print(tag.get_text())\n",
    "    title = title.replace(':','')\n",
    "    title = title.replace('\"','')\n",
    "    title = title.replace('|','')\n",
    "    title = title.replace('/','')\n",
    "    title = title.replace('\\\\','')\n",
    "    title = title.replace('*','')\n",
    "    title = title.replace('<','')\n",
    "    title = title.replace('>','')\n",
    "    title = title.replace('?','')\n",
    "    #print(tag.get_text())\n",
    "    filename = r'D:\\code\\python\\spider_news\\sina_news\\sociaty\\\\' +title+'.txt'\n",
    "    with open(filename,'w',encoding='utf8') as file_object:\n",
    "        file_object.write('           ')\n",
    "        file_object.write(title)\n",
    "        file_object.write(tag.get_text())\n",
    "    print('正在爬取第', m,'个新闻',title)\n",
    "    return 0\n",
    "if __name__ == '__main__':\n",
    "    target_url = 'http://news.sina.com.cn/society/'\n",
    "    req = request.Request(target_url)\n",
    "    response = request.urlopen(req)\n",
    "    response = response.read().decode('utf8')\n",
    "    #print(response)\n",
    "    soup = BeautifulSoup(response,'lxml')\n",
    "    #print(soup.prettify())\n",
    "    #file = open('d:\\\\test2.txt','w',encoding='utf8')\n",
    "    #file.write(soup.prettify())\n",
    "    y = 0\n",
    "    for tag in soup.find_all('div',class_='news-item'):\n",
    "        if tag.a != None:\n",
    "            if len(tag.a.string) > 8:\n",
    "                 #print(tag.a.string,tag.a.get('href'))\n",
    "                 temp = tag.a.string\n",
    "                 y += 1\n",
    "                 download(temp,tag.a.get('href'),y)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '1-1-36504939', 'column': '', 'title': '推进人工智能发展需各方联合行动', 'url': 'http://news.sina.com.cn/c/2018-11-20/doc-ihnyuqhi5168899.shtml', 'keywords': '人工智能,联合行动,肖立志', 'comment_channel': 'gn', 'img': '', 'level': '0', 'createtime': '1542722573', 'old_level': '3', 'media_type': '', 'media_name': '中国经济网'}\n",
      "正在写入csv文件中\n",
      "{'id': '1-1-36504938', 'column': '', 'title': '人民日报评论员：为世界经济拓展新的增长空间', 'url': 'http://news.sina.com.cn/o/2018-11-20/doc-ihmutuec2061859.shtml', 'keywords': '一带一路,世界经济,评论员', 'comment_channel': '', 'img': '', 'level': '0', 'createtime': '1542722528', 'old_level': '3', 'media_type': '', 'media_name': '人民日报'}\n",
      "正在写入csv文件中\n",
      "{'id': '1-1-36504936', 'column': '', 'title': '“银狐”国内告别战，球迷横幅上写着“里皮先生谢谢您”', 'url': 'http://news.sina.com.cn/c/2018-11-20/doc-ihmutuec2061390.shtml', 'keywords': '体育场,里皮,银狐', 'comment_channel': 'gn', 'img': '', 'level': '0', 'createtime': '1542722425', 'old_level': '3', 'media_type': '', 'media_name': '新京报'}\n",
      "正在写入csv文件中\n",
      "{'id': '1-1-36504932', 'column': '', 'title': '台中市长选举进入倒计时 9成新住民愿投“蓝营”', 'url': 'http://news.sina.com.cn/c/2018-11-20/doc-ihnyuqhi5163611.shtml', 'keywords': '台中,蓝营,选举', 'comment_channel': 'gn', 'img': '', 'level': '1', 'createtime': '1542722287', 'old_level': '1', 'media_type': '', 'media_name': '海外网'}\n",
      "正在写入csv文件中\n",
      "{'id': '1-1-36504926', 'column': '', 'title': '支持配合在上交所设立科创板并试点注册制，李强今调研并就这个资本市场基地建设提要求', 'url': 'http://news.sina.com.cn/c/2018-11-20/doc-ihmutuec2060553.shtml', 'keywords': '交通银行,李强,科创', 'comment_channel': 'gn', 'img': '', 'level': '0', 'createtime': '1542722095', 'old_level': '3', 'media_type': '', 'media_name': '上观'}\n",
      "正在写入csv文件中\n",
      "{'id': '1-1-36504934', 'column': '', 'title': '医院贴“高层次人才优先就诊” 网友:住33楼算吗', 'url': 'http://news.sina.com.cn/s/2018-11-20/doc-ihnyuqhi5132197.shtml', 'keywords': '医疗,就诊,医院', 'comment_channel': 'sh', 'img': '', 'level': '0', 'createtime': '1542720772', 'old_level': '2', 'media_type': '', 'media_name': '澎湃新闻'}\n",
      "正在写入csv文件中\n",
      "{'id': '1-1-36504871', 'column': '', 'title': '黄有龙赵薇等5年内不适合担任上市公司董监高', 'url': 'http://news.sina.com.cn/s/2018-11-20/doc-ihnyuqhi5126225.shtml', 'keywords': '赵薇,黄有龙,董监高', 'comment_channel': 'sh', 'img': '', 'level': '0', 'createtime': '1542720500', 'old_level': '2', 'media_type': '', 'media_name': '证券时报'}\n",
      "正在写入csv文件中\n",
      "{'id': '1-1-36504859', 'column': '', 'title': '蒋劲夫承认家暴“圈粉” 承认家暴算勇气和担当？', 'url': 'http://news.sina.com.cn/s/2018-11-20/doc-ihnyuqhi5120670.shtml', 'keywords': '蒋劲夫,勇气,暴力', 'comment_channel': 'sh', 'img': '', 'level': '0', 'createtime': '1542720251', 'old_level': '2', 'media_type': '', 'media_name': '新京报'}\n",
      "正在写入csv文件中\n",
      "{'id': '1-1-36504857', 'column': '', 'title': '黑龙江鹤岗市交巡警支队长被查 由驾驶员一路升迁', 'url': 'http://news.sina.com.cn/o/2018-11-20/doc-ihnyuqhi5119271.shtml', 'keywords': '鹤岗市,交巡警,支队长', 'comment_channel': '', 'img': '', 'level': '0', 'createtime': '1542720182', 'old_level': '2', 'media_type': '', 'media_name': '澎湃新闻'}\n",
      "正在写入csv文件中\n",
      "{'id': '1-1-36504835', 'column': '', 'title': '孝感图书馆男职工微信骚扰女子？官方：微信被盗', 'url': 'http://news.sina.com.cn/o/2018-11-20/doc-ihmutuec2051338.shtml', 'keywords': '图书馆,孝感,孝感市', 'comment_channel': '', 'img': '', 'level': '0', 'createtime': '1542719597', 'old_level': '2', 'media_type': '', 'media_name': '澎湃新闻'}\n",
      "正在写入csv文件中\n",
      "{'id': '1-1-36504882', 'column': '', 'title': '习近平出席菲律宾总统举行的欢迎仪式', 'url': 'http://news.sina.com.cn/w/2018-11-20/doc-ihnyuqhi5135084.shtml', 'keywords': '', 'comment_channel': 'gj', 'img': '', 'level': '0', 'createtime': '1542720933', 'old_level': '3', 'media_type': '', 'media_name': '央视'}\n",
      "正在写入csv文件中\n",
      "{'id': '1-1-36504842', 'column': '', 'title': '俄罗斯代表团访朝 向金正恩赠送神秘礼物(图)', 'url': 'http://news.sina.com.cn/w/2018-11-20/doc-ihnyuqhi5111172.shtml', 'keywords': '金正恩,朝鲜,俄罗斯', 'comment_channel': 'gj', 'img': '', 'level': '1', 'createtime': '1542719844', 'old_level': '1', 'media_type': '', 'media_name': '海外网'}\n",
      "正在写入csv文件中\n",
      "{'id': '1-1-36504815', 'column': '', 'title': '印度一座军火库发生爆炸 已经导致至少6人死亡', 'url': 'http://news.sina.com.cn/w/2018-11-20/doc-ihmutuec2049684.shtml', 'keywords': '军火库,印度,死亡', 'comment_channel': 'gj', 'img': '', 'level': '0', 'createtime': '1542719125', 'old_level': '3', 'media_type': '', 'media_name': '中国新闻网'}\n",
      "正在写入csv文件中\n",
      "{'id': '1-1-36504843', 'column': '', 'title': '印度一军火库发生爆炸 已造成6人死亡18人受伤', 'url': 'http://news.sina.com.cn/w/2018-11-20/doc-ihmutuec2049655.shtml', 'keywords': '军火库,受伤,印度', 'comment_channel': 'gj', 'img': '', 'level': '1', 'createtime': '1542719119', 'old_level': '1', 'media_type': '', 'media_name': '海外网'}\n",
      "正在写入csv文件中\n",
      "{'id': '1-1-36504849', 'column': '', 'title': '印度豪投5亿美元 请俄罗斯帮忙再造两艘护卫舰', 'url': 'http://news.sina.com.cn/w/2018-11-20/doc-ihmutuec2049631.shtml', 'keywords': '护卫舰,印度,俄罗斯', 'comment_channel': 'gj', 'img': '', 'level': '0', 'createtime': '1542719113', 'old_level': '2', 'media_type': '', 'media_name': '环球网'}\n",
      "正在写入csv文件中\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import threading\n",
    "import multiprocessing\n",
    "import json\n",
    "from urllib.parse import urlencode\n",
    "from urllib.request import urlopen,Request,urlparse,build_opener,install_opener\n",
    "from urllib.error import URLError,HTTPError\n",
    "\n",
    "def html_download(url):\n",
    "\n",
    "    headers = {'User-Agent': \"User-Agent:Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0)\"}\n",
    "    request = Request(url, headers=headers)\n",
    "    try:\n",
    "        html = urlopen(request).read().decode()\n",
    "    except HTTPError as e:\n",
    "        html = None\n",
    "        print('[W] 下载出现服务器错误: %s' % e.reason)\n",
    "        return None\n",
    "    except URLError as e:\n",
    "        html = None\n",
    "        print(\"[E] 站点不可达: %s\" % e.reason)\n",
    "        return None\n",
    "    return html\n",
    "\n",
    " \n",
    "def api_info_manager(page):\n",
    "    #http://api.roll.news.sina.com.cn/zt_list?channel=news&cat_1=gnxw&show_all=1&show_num=6000&tag=1&format=json\n",
    "    comment_channel = [ 'gnxw', 'shxw','gjxw']\n",
    "    for comment in comment_channel: \n",
    "\n",
    "        data= {\n",
    "            'channel':'news',\n",
    "            'cat_1':comment,\n",
    "            'show_all':1,\n",
    "            'show_num':5,\n",
    "            'tag':1,\n",
    "            'page':page,\n",
    "            'format':'json'\n",
    "            }\n",
    "        dataformat = 'http://api.roll.news.sina.com.cn/zt_list?'+urlencode(data)\n",
    "        response = html_download(dataformat)\n",
    "        #print(response)\n",
    "        json_results = json.loads(response,encoding='utf-8')['result']['data']\n",
    "\n",
    "        for info_dict in json_results:\n",
    "            print(info_dict)\n",
    "            \n",
    "            yield info_dict\n",
    " \n",
    " \n",
    "fileheader = ['id','column','title','url','keywords','comment_channel','img','level','createtime','old_level','media_type','media_name']\n",
    "\n",
    "\n",
    "def write_csv_header(fileheader):\n",
    "    with open(\"新浪新闻.csv\", \"a\", newline=\"\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fileheader)\n",
    "        writer.writeheader()\n",
    "\n",
    "    \n",
    "def save_to_csv(result):\n",
    "    with open(\"新浪新闻.csv\", \"a\", encoding = \"utf-8\", newline=\"\") as csvfile:\n",
    "        print('正在写入csv文件中')\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fileheader)\n",
    "        writer.writerow(result)\n",
    "\n",
    "\n",
    "def main(page):\n",
    "    for res in api_info_manager(page):\n",
    "        save_to_csv(res)\n",
    "\n",
    "write_csv_header(fileheader)\n",
    "main(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    #多线程  \n",
    "    write_csv_header(fileheader)   \n",
    "    pool = multiprocessing.Pool()  \n",
    "    # 多进程  \n",
    "    thread = threading.Thread(target=pool.map,args = (main,[x for x in range(1, 100)]))  \n",
    "    thread.start()  \n",
    "    thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xba in position 224738: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-4cb2bfa3a598>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"新浪新闻.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xba in position 224738: invalid start byte"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "f = open(\"新浪新闻.csv\", encoding=\"utf-8\")\n",
    "df = pd.read_csv(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
